{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOirqZ0WGvnKTVGDtiHcr+a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nivedha-M/NIVEDHA/blob/master/Pyspark_data_eng_qns_ans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.appName('creating_df').getOrCreate()\n",
        "data = [(1,'Nive','ECE',4000),\n",
        "\t(2,'vishnu','ECE',8000),\n",
        "\t(3,'shweta','IT',12000)\n",
        "\t]\n",
        "\n",
        "schema = [\"ID\",\"Name\",\"DPT\",\"Fees\"]\n",
        "spark = spark.builder.appName('creating_df').getOrCreate()\n",
        "df = spark.createDataFrame(data=data,schema=schema)\n",
        "group = df.groupBy(col('DPT')).agg(min(df.Fees).alias('Minimum_fees'),sum(df.Fees).alias('Total_fees'),count(df.DPT).alias('dpt_cnt'))\n",
        "group.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vssHas80JjJq",
        "outputId": "f16a3a91-2484-4877-c09f-81c68289e5b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+----------+-------+\n",
            "|DPT|Minimum_fees|Total_fees|dpt_cnt|\n",
            "+---+------------+----------+-------+\n",
            "|ECE|        4000|     12000|      2|\n",
            "| IT|       12000|     12000|      1|\n",
            "+---+------------+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "import pyspark.sql.types as T\n",
        "from datetime import date\n",
        "\n",
        "data = [(1,\"Nivi\",25),\n",
        "\t(2,\"vishnu\",24),\n",
        "\t(3,\"shweta\",26),\n",
        "\t(4,\"Priya\",28),\n",
        "\t(5,\"Jambu\",29)]\n",
        "\n",
        "cols = [\"ID\",\"Name\",\"age\"]\n",
        "\n",
        "df = spark.createDataFrame(data = data,schema = cols)\n",
        "current_yr = date.today().year\n",
        "usd_func = F.udf(lambda age:str(current_yr-age),T.StringType())\n",
        "new = df.withColumn('Birth_year',usd_func(df.age))\n",
        "new.show()\n",
        "\n",
        "new.select('ID','Birth_year').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elJ_cbZbKAyg",
        "outputId": "33d3d93b-60b4-44b6-d452-8eec311ab279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+----------+\n",
            "| ID|  Name|age|Birth_year|\n",
            "+---+------+---+----------+\n",
            "|  1|  Nivi| 25|      1999|\n",
            "|  2|vishnu| 24|      2000|\n",
            "|  3|shweta| 26|      1998|\n",
            "|  4| Priya| 28|      1996|\n",
            "|  5| Jambu| 29|      1995|\n",
            "+---+------+---+----------+\n",
            "\n",
            "+---+----------+\n",
            "| ID|Birth_year|\n",
            "+---+----------+\n",
            "|  1|      1999|\n",
            "|  2|      2000|\n",
            "|  3|      1998|\n",
            "|  4|      1996|\n",
            "|  5|      1995|\n",
            "+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['Customer_id' , 'Account_id', 'Account_type']\n",
        "table = [    [\"Cust_id_1\", \"accnt_id_1\", \"loan_account\"],\n",
        "    [\"Cust_id_1\", \"accnt_id_2\", \"savings_account\"],\n",
        "    [\"Cust_id_1\", \"accnt_id_3\", \"demat_account\"],\n",
        "    [\"Cust_id_2\", \"accnt_id_4\", \"loan_account\"],\n",
        "    [\"Cust_id_2\", \"accnt_id_5\", \"loan_account\"],\n",
        "    [\"Cust_id_2\", \"accnt_id_6\", \"savings_account\"],\n",
        "    [\"Cust_id_3\", \"accnt_id_7\", \"demat_account\"]\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data=table,schema=cols)\n",
        "# df.show()\n",
        "\n",
        "# +----------+-----------------+\n",
        "# |Customer_id|has_demat_account|\n",
        "# +----------+-----------------+\n",
        "# | Cust_id_1|                1|\n",
        "# | Cust_id_2|                0|\n",
        "# | Cust_id_3|                1|\n",
        "# +----------+-----------------+\n",
        "\n",
        "hi = df.groupBy('customer_id').agg(F.max(F.when(df.Account_type=='demat_account',1).otherwise(0)).alias('has_demat_account')).orderBy('Customer_id')\n",
        "hi.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U082s6HNOwoM",
        "outputId": "f7490a6c-e49c-4e6a-ed47-06731809c8ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+---------------+\n",
            "|Customer_id|Account_id|   Account_type|\n",
            "+-----------+----------+---------------+\n",
            "|  Cust_id_1|accnt_id_1|   loan_account|\n",
            "|  Cust_id_1|accnt_id_2|savings_account|\n",
            "|  Cust_id_1|accnt_id_3|  demat_account|\n",
            "|  Cust_id_2|accnt_id_4|   loan_account|\n",
            "|  Cust_id_2|accnt_id_5|   loan_account|\n",
            "|  Cust_id_2|accnt_id_6|savings_account|\n",
            "|  Cust_id_3|accnt_id_7|  demat_account|\n",
            "+-----------+----------+---------------+\n",
            "\n",
            "+-----------+-----------------+\n",
            "|customer_id|has_demat_account|\n",
            "+-----------+-----------------+\n",
            "|  Cust_id_1|                1|\n",
            "|  Cust_id_2|                0|\n",
            "|  Cust_id_3|                1|\n",
            "+-----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split,explode\n",
        "data = [('Alice','Badmiton,Tennis'),('Bob','Tennis,Cricket'),('Julie','Cricket,Carroms')]\n",
        "cols = ['name','Hobbies']\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=cols)\n",
        "# df.show()\n",
        "new = df.select('name',explode(split('Hobbies',',')).alias('exploded'))\n",
        "# new = df.explode(split('Hobbies',','))\n",
        "new.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5k1juLtXvvE",
        "outputId": "90981d36-e4ae-462a-daaa-8f8883a11425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------+\n",
            "| name|exploded|\n",
            "+-----+--------+\n",
            "|Alice|Badmiton|\n",
            "|Alice|  Tennis|\n",
            "|  Bob|  Tennis|\n",
            "|  Bob| Cricket|\n",
            "|Julie| Cricket|\n",
            "|Julie| Carroms|\n",
            "+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [('a','aa',1),\n",
        "\t('a','aa',2),\n",
        "\t('b','bb',5),\n",
        "\t('b','bb',3),\n",
        "\t('b','bb',4)]\n",
        "cols = ['col1','col2','col3']\n",
        "df = spark.createDataFrame(data=data,schema=cols)\n",
        "# df.show()\n",
        "df1 = df.groupBy('col1','col2').agg(collect_list('col3').alias('col3'))\n",
        "df1.show()\n",
        "\n",
        "print(\"using sql query\")\n",
        "\n",
        "df.createOrReplaceTempView('my_temp_view')\n",
        "df_sql = spark.sql('''\n",
        "    select col1,col2, collect_list(col3) as col3 from my_temp_view group By col1,col2\n",
        "    ''')\n",
        "df_sql.show()"
      ],
      "metadata": {
        "id": "mSsle1rZZNOG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd2f1a58-4438-4a29-b58c-6e3ba3019a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+---------+\n",
            "|col1|col2|     col3|\n",
            "+----+----+---------+\n",
            "|   a|  aa|   [1, 2]|\n",
            "|   b|  bb|[5, 3, 4]|\n",
            "+----+----+---------+\n",
            "\n",
            "using sql query\n",
            "+----+----+---------+\n",
            "|col1|col2|     col3|\n",
            "+----+----+---------+\n",
            "|   a|  aa|   [1, 2]|\n",
            "|   b|  bb|[5, 3, 4]|\n",
            "+----+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [{'dpt_id':'101','e_id':[10101,10102,10103]},\n",
        "{'dpt_id':'102','e_id':[10201,10202]}]\n",
        "\n",
        "df = spark.createDataFrame(data=data)\n",
        "df.show()\n",
        "\n",
        "df1 = df.select('dpt_id',explode('e_id').alias('e_id'))\n",
        "df1.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Cqznwa-nEsw",
        "outputId": "c45b84d0-5fa1-4a15-b312-6f56e53a863d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+\n",
            "|dpt_id|                e_id|\n",
            "+------+--------------------+\n",
            "|   101|[10101, 10102, 10...|\n",
            "|   102|      [10201, 10202]|\n",
            "+------+--------------------+\n",
            "\n",
            "+------+-----+\n",
            "|dpt_id| e_id|\n",
            "+------+-----+\n",
            "|   101|10101|\n",
            "|   101|10102|\n",
            "|   101|10103|\n",
            "|   102|10201|\n",
            "|   102|10202|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7)create df in pyspark.Find avg stock value on daily basis of each stock.Find max avg stock value of each stock.\n",
        "\n",
        "data = [('2023-01-01','AAPL',150.00),('2023-01-02','AAPL',155.00),\n",
        "        ('2023-01-01','AAPL',200.00),('2023-01-01','MSFT',400.00),\n",
        "\t('2023-01-01','GOOG',2500.00),('2023-01-02','GOOG',2550.00),\n",
        "\t('2023-01-01','MSFT',300.00),('2023-01-02','MSFT',310.00)]\n",
        "cols = ['date','stock','value']\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=cols)\n",
        "df = df.withColumn('date',to_date('date'))\n",
        "avg_daily_basis = df.groupBy('date','stock').agg(avg('value').alias('avg_value'))\n",
        "max_avg_each_stock = avg_daily_basis.groupBy('stock').agg(max('avg_value').alias('max_avg_value'))\n",
        "max_avg_each_stock.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGn_2Pn5pKW0",
        "outputId": "8d3f3ec4-f4e2-4328-f27f-b55737ba99ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------+\n",
            "|stock|max_avg_value|\n",
            "+-----+-------------+\n",
            "| AAPL|        175.0|\n",
            "| GOOG|       2550.0|\n",
            "| MSFT|        350.0|\n",
            "+-----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "table1 = [(1,'steve'),(2,'David'),(3,'Aryan')]\n",
        "schema1 = ['student_id','student_name']\n",
        "\n",
        "table2 = [(1,'pyspark','90'),(1,'sql',100),(2,'sql',70),(2,'pyspark',60),(3,'sql',30),(2,'pyspark',20)]\n",
        "schema2 = ['student_id','subject_name','marks']\n",
        "\n",
        "\n",
        "#OUTPUT:\n",
        "# student_id\tstudent_name\tpercentage\tgrade\n",
        "# 1\t\tSteve\t\t95\t\tDistinction\n",
        "# 2\t\tDavid\t\t50\t\tSecond Class\n",
        "# 3\t\tAryan\t\t30\t\tFail\n",
        "\n",
        "df1 = spark.createDataFrame(data=table1,schema=schema1)\n",
        "df2 = spark.createDataFrame(data=table2,schema=schema2)\n",
        "\n",
        "joined_df = df1.join(df2,df1.student_id==df2.student_id).drop(df2.student_id)\n",
        "# joined_df.show()\n",
        "new = joined_df.groupby('student_id','student_name').agg(avg('marks').alias('percentage'))\n",
        "added_grade = new.select('*',when(new.percentage>=70,'Distinction')\n",
        "                             .when((new.percentage<70)&(new.percentage>=60),'First Class')\n",
        "                             .when((new.percentage<60)&(new.percentage>=50),'Second Class')\n",
        "                             .when((new.percentage<50)&(new.percentage>=40),'Third Class')\n",
        "                             .when(new.percentage<40,'Fail').alias('grade')\n",
        "                             )\n",
        "added_grade.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TR9XIPMHpkkX",
        "outputId": "d6980f78-1cae-4180-8563-8e60823147a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+----------+------------+\n",
            "|student_id|student_name|percentage|       grade|\n",
            "+----------+------------+----------+------------+\n",
            "|         1|       steve|      95.0| Distinction|\n",
            "|         2|       David|      50.0|Second Class|\n",
            "|         3|       Aryan|      30.0|        Fail|\n",
            "+----------+------------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the delimiters in a file pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, col\n",
        "\n",
        "spark = SparkSession.builder.appName('Data_Engineering_prbmls').getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\"1,Alice\\t30|New York\"]\n",
        "\n",
        "df = spark.createDataFrame(data,\"string\")\n",
        "splitted_value = split(df['value'],',|\\t|\\|')\n",
        "df = df.withColumn('id',splitted_value.getItem(0))\\\n",
        "       .withColumn('Name',splitted_value.getItem(1))\\\n",
        "       .withColumn('age',splitted_value.getItem(2))\\\n",
        "       .withColumn('Country',splitted_value.getItem(3))\n",
        "df.select('id','Name','age','Country').show()\n",
        "df.show()"
      ],
      "metadata": {
        "id": "5gxRYH-XrK_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8463e70c-8e44-447d-f2e2-00f9eb849cbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---+--------+\n",
            "| id| Name|age| Country|\n",
            "+---+-----+---+--------+\n",
            "|  1|Alice| 30|New York|\n",
            "+---+-----+---+--------+\n",
            "\n",
            "+--------------------+---+-----+---+--------+\n",
            "|               value| id| Name|age| Country|\n",
            "+--------------------+---+-----+---+--------+\n",
            "|1,Alice\\t30|New York|  1|Alice| 30|New York|\n",
            "+--------------------+---+-----+---+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MISSING VALUE FROM THE LIST\n",
        "from pyspark.sql.functions import col\n",
        "data = [(1,), (2,), (4,), (5,), (7,), (8,), (10,),(11,)]\n",
        "df = spark.createDataFrame(data=data,schema = [\"Numbers\"])\n",
        "# df.show()\n",
        "range_num = spark.range(1,11).toDF(\"Numbers\")\n",
        "# range_num.show()\n",
        "new = range_num.join(df,\"Numbers\",\"left\")\n",
        "new.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9bGFfLP5QzC",
        "outputId": "e5e0b033-985f-4564-ce9f-6aa669433025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+\n",
            "|Numbers|\n",
            "+-------+\n",
            "|      5|\n",
            "|      1|\n",
            "|      3|\n",
            "|      2|\n",
            "|      4|\n",
            "|      7|\n",
            "|      6|\n",
            "|      9|\n",
            "|     10|\n",
            "|      8|\n",
            "+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum,count\n",
        "from pyspark.sql.window import WindowSpec\n",
        "data_movies = [(1, \"Movie A\"), (2, \"Movie B\"), (3, \"Movie C\"), (4, \"Movie D\"), (5, \"Movie E\")]\n",
        "\n",
        "data_ratings = [(1, 101, 4.5), (1, 102, 4.0), (2, 103, 5.0),\n",
        "                (2, 104, 3.5), (3, 105, 4.0), (3, 106, 4.0),\n",
        "                (4, 107, 3.0), (5, 108, 2.5), (5, 109, 3.0)]\n",
        "\n",
        "columns_movies = [\"MovieID\", \"MovieName\"]\n",
        "columns_ratings = [\"MovieID\", \"UserID\", \"Rating\"]\n",
        "\n",
        "df1 = spark.createDataFrame(data=data_movies,schema=columns_movies)\n",
        "df2 = spark.createDataFrame(data=data_ratings,schema=columns_ratings)\n",
        "\n",
        "avg_cnt_ratings = df2.groupBy('MovieID').agg((sum(col('Rating'))/count(col('Rating'))).alias('Avg_rating'))\n",
        "top_3_rating = avg_cnt_ratings.orderBy(col(\"Avg_rating\"),ascending=False).limit(3)\n",
        "new = df1.join(top_3_rating,df1.MovieID==top_3_rating.MovieID,'inner').drop(top_3_rating.MovieID)\n",
        "new.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuWa3RkI837m",
        "outputId": "dc30ff5d-f522-4e4e-835d-22c203c4b5da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+----------+\n",
            "|MovieID|MovieName|Avg_rating|\n",
            "+-------+---------+----------+\n",
            "|      1|  Movie A|      4.25|\n",
            "|      2|  Movie B|      4.25|\n",
            "|      3|  Movie C|       4.0|\n",
            "+-------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,avg\n",
        "data_movies = [(1, \"Movie A\"), (2, \"Movie B\"), (3, \"Movie C\"), (4, \"Movie D\"), (5, \"Movie E\")]\n",
        "\n",
        "data_ratings = [(1, 101, 4.5), (1, 102, 4.0), (2, 103, 5.0),\n",
        "                (2, 104, 3.5), (3, 105, 4.0), (3, 106, 4.0),\n",
        "                (4, 107, 3.0), (5, 108, 2.5), (5, 109, 3.0)]\n",
        "\n",
        "columns_movies = [\"MovieID\", \"MovieName\"]\n",
        "columns_ratings = [\"MovieID\", \"UserID\", \"Rating\"]\n",
        "\n",
        "df1 = spark.createDataFrame(data = data_movies,schema=columns_movies)\n",
        "df2 = spark.createDataFrame(data = data_ratings,schema=columns_ratings)\n",
        "\n",
        "avg_rating = df2.groupBy(col('MovieID')).agg((avg('Rating')).alias('Avg_rating'))\n",
        "joined = df1.join(avg_rating,'MovieID','inner').orderBy(\"Avg_rating\",ascending=False).limit(3)\n",
        "joined.select(\"MovieID\",\"MovieName\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwJRJk-UFMDS",
        "outputId": "5cdb2861-9c0d-4915-f980-4e192099d268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|MovieID|MovieName|\n",
            "+-------+---------+\n",
            "|      1|  Movie A|\n",
            "|      2|  Movie B|\n",
            "|      3|  Movie C|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "from pyspark.sql import Row,SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "data = [Row(Date='2023-01-01', ProductID=100, QuantitySold=10),\n",
        "        Row(Date='2023-01-02', ProductID=100, QuantitySold=15),\n",
        "        Row(Date='2023-01-03', ProductID=100, QuantitySold=20),\n",
        "        Row(Date='2023-01-04', ProductID=100, QuantitySold=25),\n",
        "        Row(Date='2023-01-05', ProductID=100, QuantitySold=30),\n",
        "        Row(Date='2023-01-06', ProductID=100, QuantitySold=35),\n",
        "        Row(Date='2023-01-07', ProductID=100, QuantitySold=40),\n",
        "        Row(Date='2023-01-08', ProductID=100, QuantitySold=45)]\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "df = df.withColumn(\"Date\",F.to_date(df.Date))\n",
        "windowSpec = Window.partitionBy('ProductID').orderBy('Date').rowsBetween(-6,0)\n",
        "df = df.withColumn('7 days Avg',F.avg('QuantitySold').over(windowSpec))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VrmDCwsLuuz",
        "outputId": "0149e466-d3f2-4651-86bb-00a7476cebb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+------------+----------+\n",
            "|      Date|ProductID|QuantitySold|7 days Avg|\n",
            "+----------+---------+------------+----------+\n",
            "|2023-01-01|      100|          10|      10.0|\n",
            "|2023-01-02|      100|          15|      12.5|\n",
            "|2023-01-03|      100|          20|      15.0|\n",
            "|2023-01-04|      100|          25|      17.5|\n",
            "|2023-01-05|      100|          30|      20.0|\n",
            "|2023-01-06|      100|          35|      22.5|\n",
            "|2023-01-07|      100|          40|      25.0|\n",
            "|2023-01-08|      100|          45|      30.0|\n",
            "+----------+---------+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row,SparkSession\n",
        "from pyspark.sql.types import StringType\n",
        "import pyspark.sql.functions as F\n",
        "data = [Row(UserID=4001, Age=17),\n",
        "        Row(UserID=4002, Age=45),\n",
        "        Row(UserID=4003, Age=65),\n",
        "        Row(UserID=4004, Age=30),\n",
        "        Row(UserID=4005, Age=80)]\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "def age_group(age):\n",
        "  if age<=17:\n",
        "    return \"Youth\"\n",
        "  elif age>17 and age<=45:\n",
        "    return \"Adult\"\n",
        "  else:\n",
        "    return \"senior\"\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "#F.udf(my_udf, returnType=DataType()) - syntax of udf\n",
        "func_obj_udf = F.udf(age_group,StringType())\n",
        "df = df.withColumn('Age_group',func_obj_udf('age'))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS3mlYyGRMxm",
        "outputId": "e18ac6c4-3f00-4b6f-e84d-c941fb3e39ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+---------+\n",
            "|UserID|Age|Age_group|\n",
            "+------+---+---------+\n",
            "|  4001| 17|    Youth|\n",
            "|  4002| 45|    Adult|\n",
            "|  4003| 65|   senior|\n",
            "|  4004| 30|    Adult|\n",
            "|  4005| 80|   senior|\n",
            "+------+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GET UNIQUE VISITORS\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import count_distinct\n",
        "visitor_data = [Row(Date='2023-01-01', VisitorID=101),\n",
        "                Row(Date='2023-01-01', VisitorID=102),\n",
        "                Row(Date='2023-01-01', VisitorID=101),\n",
        "                Row(Date='2023-01-02', VisitorID=103),\n",
        "                Row(Date='2023-01-02', VisitorID=101)]\n",
        "\n",
        "df = spark.createDataFrame(visitor_data)\n",
        "df = df.withColumn(\"Date\",F.to_date(\"Date\"))\n",
        "df = df.groupBy('Date').agg(count_distinct('VisitorID').alias('unique_visitors'))\n",
        "df.show()"
      ],
      "metadata": {
        "id": "XzXqYrgiVzOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e940f7b2-e5f5-4033-a07d-c04229ca4935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------------+\n",
            "|      Date|unique_visitors|\n",
            "+----------+---------------+\n",
            "|2023-01-01|              2|\n",
            "|2023-01-02|              2|\n",
            "+----------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FIRST PURCHASE DATE OF EACH USER\n",
        "from pyspark.sql.functions import *\n",
        "purchase_data = [\n",
        "    Row(UserID=1, PurchaseDate='2023-01-05'),\n",
        "    Row(UserID=1, PurchaseDate='2023-01-10'),\n",
        "    Row(UserID=2, PurchaseDate='2023-01-03'),\n",
        "    Row(UserID=3, PurchaseDate='2023-01-12')\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(purchase_data)\n",
        "df = df.withColumn(\"PurchaseDate\",to_date(\"PurchaseDate\"))\n",
        "df = df.groupBy(\"UserID\").agg(min('PurchaseDate').alias(\"first_purchase_date\")).orderBy('UserID')\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVMBA6fRG5hC",
        "outputId": "da84303a-68ff-4504-f969-05aa904c3d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------------+\n",
            "|UserID|first_purchase_date|\n",
            "+------+-------------------+\n",
            "|     1|         2023-01-05|\n",
            "|     2|         2023-01-03|\n",
            "|     3|         2023-01-12|\n",
            "+------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate Sequential order of each row in each group\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "group_data = [\n",
        "    Row(GroupID='A', Date='2023-01-01'),\n",
        "    Row(GroupID='A', Date='2023-01-02'),\n",
        "    Row(GroupID='B', Date='2023-01-01'),\n",
        "    Row(GroupID='B', Date='2023-01-03')\n",
        "]\n",
        "df = spark.createDataFrame(group_data)\n",
        "df = df.withColumn('Date',to_date('Date'))\n",
        "windowspec = Window.partitionBy('GroupID').orderBy('Date')\n",
        "df = df.withColumn('sequential_num',row_number().over(windowspec))\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFf86QnyKjjg",
        "outputId": "2960ab7c-d46e-479b-857d-4cb92a19744e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+--------------+\n",
            "|GroupID|      Date|sequential_num|\n",
            "+-------+----------+--------------+\n",
            "|      A|2023-01-01|             1|\n",
            "|      A|2023-01-02|             2|\n",
            "|      B|2023-01-01|             1|\n",
            "|      B|2023-01-03|             2|\n",
            "+-------+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "sales_data = [(\"1\", 100), (\"2\", 150), (\"3\", None), (\"4\", 200), (\"5\", None)]\n",
        "schema = [\"sales_id\",\"amount\"]\n",
        "df = spark.createDataFrame(sales_data,schema)\n",
        "# avg_here = df.dropna(subset=['amount']).agg(avg('amount').alias('avg_amt')).first()[0]\n",
        "avg_here = df.na.drop().agg(avg(\"amount\").alias('avg_amt')).first()[0]\n",
        "print(avg_here)\n",
        "# avg_here.show()\n",
        "# all = df.fillna(avg_here)\n",
        "# all.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2kONnlXMmAZ",
        "outputId": "94602842-6f6d-4660-9b93-d05097202cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "150.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PIVOT the dataframe\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "data = [(\"Product1\", 100, 150, 200),\n",
        "        (\"Product2\", 200, 250, 300),\n",
        "        (\"Product3\", 300, 350, 400)]\n",
        "\n",
        "# Columns: Product, Sales_Jan, Sales_Feb, Sales_Mar\n",
        "columns = [\"Product\", \"Sales_Jan\", \"Sales_Feb\", \"Sales_Mar\"]\n",
        "spark = SparkSession.builder.appName(\"pivoting_table\").getOrCreate()\n",
        "df = spark.createDataFrame(data,columns)\n",
        "df.show()\n",
        "\n",
        "pivoted_table = df.selectExpr('Product',\"stack(3,'jan',Sales_Jan,'Feb',Sales_Feb,'Mar',Sales_Mar) as (Month,Sales)\")\n",
        "pivoted_table.show()"
      ],
      "metadata": {
        "id": "qepAPB4-QNbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd96bdb8-f5c3-464a-d96e-1439667eeb63"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+---------+---------+\n",
            "| Product|Sales_Jan|Sales_Feb|Sales_Mar|\n",
            "+--------+---------+---------+---------+\n",
            "|Product1|      100|      150|      200|\n",
            "|Product2|      200|      250|      300|\n",
            "|Product3|      300|      350|      400|\n",
            "+--------+---------+---------+---------+\n",
            "\n",
            "+--------+-----+-----+\n",
            "| Product|Month|Sales|\n",
            "+--------+-----+-----+\n",
            "|Product1|  jan|  100|\n",
            "|Product1|  Feb|  150|\n",
            "|Product1|  Mar|  200|\n",
            "|Product2|  jan|  200|\n",
            "|Product2|  Feb|  250|\n",
            "|Product2|  Mar|  300|\n",
            "|Product3|  jan|  300|\n",
            "|Product3|  Feb|  350|\n",
            "|Product3|  Mar|  400|\n",
            "+--------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1LjLuRue9-eM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AzDNwUSZBHHv"
      }
    }
  ]
}